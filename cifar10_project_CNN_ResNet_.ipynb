{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cifar10_project_CNN_ResNet .ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaushikRoy94/Work/blob/master/cifar10_project_CNN_ResNet_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pmn8OECbBsGV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "898488ca-6986-4927-dabe-af509b4a5d1e"
      },
      "source": [
        "#!/usr/bin/env python\n",
        "\n",
        "\"\"\"\n",
        "    convnet/prep.py\n",
        "    \n",
        "    Grabs the first two classes of CIFAR10 and saves them as numpy arrays\n",
        "    \n",
        "    Since we don't assume everyone has access to GPUs, we do this to create\n",
        "    a dataset that can be trained in a reasonable amount of time on a CPU.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import json\n",
        "import argparse\n",
        "import math\n",
        "from math import ceil\n",
        "from time import time\n",
        "from keras.initializers import RandomUniform\n",
        "import torch\n",
        "import keras.backend as K\n",
        "from torchvision import transforms, datasets\n",
        "import numpy as np\n",
        "from keras import layers\n",
        "import keras\n",
        "from keras.losses import sparse_categorical_crossentropy\n",
        "from keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D\n",
        "from keras.models import Model, load_model\n",
        "from keras.preprocessing import image\n",
        "from keras.utils import layer_utils\n",
        "from keras.utils.data_utils import get_file\n",
        "from keras.applications.imagenet_utils import preprocess_input\n",
        "import pydot\n",
        "from IPython.display import SVG\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "from keras.utils import plot_model\n",
        "import scipy.misc\n",
        "from matplotlib.pyplot import imshow\n",
        "%matplotlib inline\n",
        "from keras.optimizers import SGD\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.utils.vis_utils import plot_model\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    \n",
        "    # --\n",
        "    # Load data\n",
        "    \n",
        "    print('prep.py: dowloading cifar10', file=sys.stderr)\n",
        "    trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
        "    testset  = datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
        "    \n",
        "    X_train, y_train = zip(*[(x, y) for x, y in trainset if y <= 1])\n",
        "    X_test, y_test   = zip(*[(x, y) for x, y in testset if y <= 1])\n",
        "    \n",
        "    X_train = np.array(torch.stack(X_train)).astype(np.float32)\n",
        "    X_test  = np.array(torch.stack(X_test)).astype(np.float32)\n",
        "    y_train = np.array(y_train).astype(np.int64)\n",
        "    y_test  = np.array(y_test).astype(np.int64)\n",
        "    \n",
        "    # --\n",
        "    # Scale and center data\n",
        "    \n",
        "    X_mean = X_train.transpose(1, 0, 2, 3).reshape(3, -1).mean(axis=-1).reshape(1, 3, 1, 1)\n",
        "    X_std = X_train.transpose(1, 0, 2, 3).reshape(3, -1).std(axis=-1).reshape(1, 3, 1, 1)\n",
        "    \n",
        "    X_train = (X_train - X_mean) / X_std\n",
        "    X_test  = (X_test - X_mean) / X_std\n",
        "    \n",
        "    # --\n",
        "    # Save to file\n",
        "    \n",
        "    os.makedirs('data/cifar2', exist_ok=True)\n",
        "    \n",
        "    print('prep.py: saving to data/cifar2', file=sys.stderr)\n",
        "    np.save('data/cifar2/X_train.npy', X_train)\n",
        "    np.save('data/cifar2/X_test.npy', X_test)\n",
        "    \n",
        "    np.save('data/cifar2/y_train.npy', y_train)\n",
        "    np.save('data/cifar2/y_test.npy', y_test)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "prep.py: dowloading cifar10\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "prep.py: saving to data/cifar2\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kehf7f8pB-87",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        },
        "outputId": "f68e3b05-2930-477c-b51c-5ed0633ad530"
      },
      "source": [
        "\n",
        "\"\"\"\n",
        "    convnet/main.py\n",
        "\"\"\"\n",
        "\n",
        "# --\n",
        "# User code\n",
        "# Note: Depending on how you implement your model, you'll likely have to change the parameters of these\n",
        "# functions.  They way their shown is just one possble way that the code could be structured.\n",
        " \n",
        "\n",
        "def identity_block(X, filters, input_channels):\n",
        "    \n",
        "    # Save the input value\n",
        "    X_shortcut = X\n",
        "\n",
        "    # Component of main path\n",
        "    scale = math.sqrt(1 / (9 * input_channels))\n",
        "    X = ZeroPadding2D((1,1))(X)\n",
        "    X = Conv2D(filters=filters, kernel_size=(3, 3), strides=(1, 1), kernel_initializer=RandomUniform(minval=-scale, maxval=scale, seed=None), use_bias=False)(X)\n",
        "    print(X.shape)\n",
        "    X = BatchNormalization( momentum=0.99,epsilon=0.001, beta_initializer='zeros', gamma_initializer='ones',  moving_mean_initializer='zeros', moving_variance_initializer='ones')(X)\n",
        "    X = Activation('relu')(X)\n",
        "    \n",
        "    # Final step: Add shortcut value to main path\n",
        "    X = Add()([X, X_shortcut])\n",
        "\n",
        "    return X\n",
        "  \n",
        "def convolutional_block(X, filters, input_channels):\n",
        "\n",
        "    # Save the input value\n",
        "    X_shortcut = X\n",
        "\n",
        "    # Component of main path\n",
        "    scale = math.sqrt(1 / (9 * input_channels))\n",
        "    X = ZeroPadding2D((1,1))(X)\n",
        "    X = Conv2D(filters=filters, kernel_size=(3, 3), strides=(1, 1), kernel_initializer=RandomUniform(minval=-scale, maxval=scale, seed=None), use_bias=False)(X)\n",
        "    print(X.shape)\n",
        "    X = BatchNormalization( momentum=0.99,epsilon=0.001, beta_initializer='zeros', gamma_initializer='ones',  moving_mean_initializer='zeros', moving_variance_initializer='ones')(X)\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    # Shortcut path\n",
        "    scale = math.sqrt(1 / ( input_channels))\n",
        "    X_shortcut = Conv2D(filters=filters, kernel_size=(1, 1), strides=(1, 1), padding='valid', kernel_initializer=RandomUniform(minval=-scale, maxval=scale, seed=None), use_bias=False)(X_shortcut)\n",
        "    \n",
        "    # Final step: Add shortcut value to main path\n",
        "    X = Add()([X, X_shortcut])\n",
        "    \n",
        "    return X\n",
        "  \n",
        "  \n",
        " \n",
        "def make_model(input_channels, output_classes, scale_alpha, optimizer, lr, momentum):\n",
        "\n",
        "    # ... your code here ...\n",
        "  \n",
        "    image_shape = (32, 32, input_channels)\n",
        "    inputs = Input(image_shape)\n",
        "    \n",
        "    # Stage 1\n",
        "    scale = math.sqrt(1 / (9 * input_channels))\n",
        "    x=ZeroPadding2D((1,1))(inputs)\n",
        "    x = Conv2D(filters=16, kernel_size=(3, 3), strides=(1, 1), kernel_initializer=RandomUniform(minval=-scale, maxval=scale, seed=None), use_bias=False)(x)\n",
        "    print(x.shape)\n",
        "    x = BatchNormalization( momentum=0.99,epsilon=0.001, beta_initializer='zeros', gamma_initializer='ones',  moving_mean_initializer='zeros', moving_variance_initializer='ones')(x)\n",
        "    x = Activation('relu')(x)\n",
        "    \n",
        "    # Stage 2\n",
        "    x = convolutional_block(x, filters=32, input_channels=16)\n",
        "    x = identity_block(x, filters=32, input_channels=16)\n",
        "    \n",
        "    # Stage 3\n",
        "    print(\"x before stage 3\",x.shape)\n",
        "    x = MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
        "    print(\"x after stage 3\",x.shape)\n",
        "    \n",
        "    # Stage 4\n",
        "    x = convolutional_block(x, filters=64, input_channels=32)\n",
        "    x = identity_block(x, filters=64, input_channels=32)\n",
        "    \n",
        "    # Stage 5\n",
        "    print(\"x before stage 5\",x.shape)\n",
        "    x = MaxPooling2D((2, 2), strides=(2, 2))(x)\n",
        "    print(\"x after stage 3\",x.shape)\n",
        "    \n",
        "    # Stage 6\n",
        "    x = convolutional_block(x, filters=128, input_channels=64)\n",
        "    x = identity_block(x, filters=128, input_channels=64)\n",
        "    \n",
        "    # Stage 7\n",
        "    (_,x_h,x_w,x_c)=x.shape\n",
        "    x = GlobalMaxPooling2D()(x)\n",
        " \n",
        "    # Stage 8\n",
        "    _,input_dim = x.shape\n",
        "    print(\"input dim=\",input_dim)\n",
        "    scale = math.sqrt(1 /128)\n",
        "    x = Dense(output_classes, activation='linear',kernel_initializer=RandomUniform(minval=-scale, maxval=scale, seed=None), use_bias=False)(x)\n",
        "    \n",
        "    # Stage 9\n",
        "    print(x.shape)\n",
        "    #x = x[:,0]*scale_alpha\n",
        "    #x = x[:,1]*scale_alpha\n",
        "    \n",
        "    # Output layer\n",
        "    x = Activation('softmax')(x)\n",
        "    \n",
        "    # Create model\n",
        "    model = Model(inputs, x)\n",
        " \n",
        "    if optimizer == \"SGD\":\n",
        "        sgd = SGD(lr=lr, momentum=momentum)\n",
        "        model.compile(optimizer=sgd, loss=sparse_categorical_crossentropy,\n",
        "            metrics=['accuracy'])\n",
        "    else:\n",
        "        model.compile(optimizer, loss=sparse_categorical_crossentropy,\n",
        "                metrics=['accuracy'])\n",
        "    return model\n",
        " \n",
        " \n",
        "def make_train_dataloader(X, y, batch_size, shuffle):\n",
        "    # ... your code here ...\n",
        "    data = ImageDataGenerator()\n",
        "    dataloader = data.flow(X, y, batch_size, shuffle=shuffle)\n",
        "    return dataloader\n",
        " \n",
        " \n",
        "def make_test_dataloader(X, batch_size, shuffle):\n",
        "    # ... your code here ...\n",
        "    data1 = ImageDataGenerator()\n",
        "    dataloader = data1.flow(X, None, batch_size, shuffle=shuffle)\n",
        "    return dataloader\n",
        " \n",
        " \n",
        "def train_one_epoch(model, dataloader, steps):\n",
        "    # ... your code here ...\n",
        "    model.fit_generator(dataloader, epochs=1,\n",
        "            steps_per_epoch=steps)\n",
        "    return model\n",
        " \n",
        " \n",
        "def predict(model, dataloader, steps):\n",
        "    # ... your code here ...\n",
        "    probablities = model.predict_generator(dataloader,\n",
        "            steps=steps)\n",
        "    predictions = []\n",
        " \n",
        "    for i in range(probablities.shape[0]):\n",
        "        max_probablity = -1\n",
        "        best_class = -1\n",
        "        for j in range(probablities.shape[1]):\n",
        "            if probablities[i][j] > max_probablity:\n",
        "                max_probablity = probablities[i][j]\n",
        "                best_class = j\n",
        "        predictions.append(best_class)\n",
        "    predictions = np.array(predictions)\n",
        "    return predictions\n",
        " \n",
        "# --\n",
        "# CLI\n",
        " \n",
        "#def parse_args():\n",
        "#    parser = argparse.ArgumentParser()\n",
        "#    parser.add_argument('--cuda', action=\"store_true\")\n",
        "#    parser.add_argument('--num-epochs', type=int, default=5)\n",
        "#    parser.add_argument('--lr', type=float, default=0.1)\n",
        "#    parser.add_argument('--momentum', type=float, default=0.9)\n",
        "#    parser.add_argument('--batch-size', type=int, default=128)\n",
        "#    return parser.parse_args()\n",
        " \n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    #args = parse_args()\n",
        "   \n",
        "    # --\n",
        "    # IO\n",
        "   \n",
        "    # X_train: tensor of shape (number of train observations, number of image channels, image height, image width)\n",
        "    # X_test:  tensor of shape (number of train observations, number of image channels, image height, image width)\n",
        "    # y_train: vector of [0, 1] class labels for each train image\n",
        "    # y_test:  vector of [0, 1] class labels for each test image (don't look at these to make predictions!)\n",
        "   \n",
        "    X_train = np.load('data/cifar2/X_train.npy')\n",
        "    X_test  = np.load('data/cifar2/X_test.npy')\n",
        "    y_train = np.load('data/cifar2/y_train.npy')\n",
        "    y_test  = np.load('data/cifar2/y_test.npy')\n",
        "    X_train = np.transpose(X_train, (0, 2, 3, 1))\n",
        "    X_test = np.transpose(X_test, (0, 2, 3, 1))\n",
        " \n",
        "    # --\n",
        "    # Define model\n",
        "   \n",
        "    model = make_model(\n",
        "        input_channels=3,\n",
        "        output_classes=2,\n",
        "        scale_alpha=0.125,\n",
        "        optimizer=\"SGD\",\n",
        "        lr=0.006,\n",
        "        momentum=0.9,\n",
        "    )\n",
        "    \n",
        "    # --\n",
        "    # Train\n",
        "        \n",
        "    t = time()\n",
        "    for epoch in range(5):\n",
        "       \n",
        "        # Train\n",
        "        model = train_one_epoch(\n",
        "            model=model,\n",
        "            dataloader=make_train_dataloader(X_train, y_train, batch_size=128, shuffle=True),\n",
        "            steps=ceil(X_train.shape[0] / 128)\n",
        "        )\n",
        "       \n",
        "        # Evaluate\n",
        "        preds = predict(\n",
        "            model=model,\n",
        "            dataloader=make_test_dataloader(X_test, batch_size=128, shuffle=False),\n",
        "            steps=ceil(X_test.shape[0] / 128)\n",
        "        )\n",
        "       \n",
        "        assert isinstance(preds, np.ndarray)\n",
        "        assert preds.shape[0] == X_test.shape[0]\n",
        "       \n",
        "        test_acc = (preds == y_test.squeeze()).mean()\n",
        "       \n",
        "        print(json.dumps({\n",
        "            \"epoch\"    : int(epoch),\n",
        "            \"test_acc\" : test_acc,\n",
        "            \"time\"     : time() - t\n",
        "        }))\n",
        "        sys.stdout.flush()\n",
        "       \n",
        "    elapsed = time() - t\n",
        "    print('elapsed', elapsed, file=sys.stderr)\n",
        "   \n",
        "    # --\n",
        "    # Save results\n",
        "   \n",
        "    os.makedirs('results', exist_ok=True)\n",
        "   \n",
        "    np.savetxt('results/preds', preds, fmt='%d')\n",
        "    open('results/elapsed', 'w').write(str(elapsed))"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(?, 32, 32, 16)\n",
            "(?, 32, 32, 32)\n",
            "(?, 32, 32, 32)\n",
            "x before stage 3 (?, 32, 32, 32)\n",
            "x after stage 3 (?, 16, 16, 32)\n",
            "(?, 16, 16, 64)\n",
            "(?, 16, 16, 64)\n",
            "x before stage 5 (?, 16, 16, 64)\n",
            "x after stage 3 (?, 8, 8, 64)\n",
            "(?, 8, 8, 128)\n",
            "(?, 8, 8, 128)\n",
            "input dim= 128\n",
            "(?, 2)\n",
            "Epoch 1/1\n",
            "79/79 [==============================] - 12s 149ms/step - loss: 0.4531 - acc: 0.8229\n",
            "{\"epoch\": 0, \"test_acc\": 0.912, \"time\": 16.279667377471924}\n",
            "Epoch 1/1\n",
            "79/79 [==============================] - 2s 29ms/step - loss: 0.1983 - acc: 0.9194\n",
            "{\"epoch\": 1, \"test_acc\": 0.7595, \"time\": 18.713507890701294}\n",
            "Epoch 1/1\n",
            "79/79 [==============================] - 2s 28ms/step - loss: 0.1191 - acc: 0.9530\n",
            "{\"epoch\": 2, \"test_acc\": 0.905, \"time\": 21.091830730438232}\n",
            "Epoch 1/1\n",
            "79/79 [==============================] - 2s 28ms/step - loss: 0.0786 - acc: 0.9712\n",
            "{\"epoch\": 3, \"test_acc\": 0.927, \"time\": 23.486514806747437}\n",
            "Epoch 1/1\n",
            "79/79 [==============================] - 2s 28ms/step - loss: 0.0492 - acc: 0.9814\n",
            "{\"epoch\": 4, \"test_acc\": 0.96, \"time\": 25.874765157699585}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "elapsed 25.877500772476196\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ztnn8E1WraKr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOxhE26MaOuh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fd8336d2-adc4-4ef3-8a17-4b4f66ec8409"
      },
      "source": [
        "\n",
        "\"\"\"\n",
        "    convnet/validate.py\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "ACC_THRESHOLD = 0.95\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    y_test = np.load('data/cifar2/y_test.npy')\n",
        "    y_pred = np.array([int(xx) for xx in open('results/preds').read().splitlines()])\n",
        "    \n",
        "    test_acc = (y_test == y_pred).mean()\n",
        "    \n",
        "    # --\n",
        "    # Log\n",
        "    \n",
        "    print(json.dumps({\n",
        "        \"test_acc\" : float(test_acc),\n",
        "        \"status\"   : \"PASS\" if test_acc >= ACC_THRESHOLD else \"FAIL\",\n",
        "    }))\n",
        "    \n",
        "    does_pass = \"PASS\" if test_acc >= ACC_THRESHOLD else \"FAIL\"\n",
        "    open('results/.pass', 'w').write(str(does_pass))"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\"test_acc\": 0.96, \"status\": \"PASS\"}\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}